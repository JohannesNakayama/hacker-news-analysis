geom_tile() +
labs(title = "Gained votes on new vs. top page",
x = "newpage rank",
y = "toppage rank",
fill = "gained votes") +
scale_y_reverse() +
coord_fixed() +
scale_fill_viridis_c() +
# scale_fill_gradient(low = "darkblue", high = "yellow") +
# scale_fill_gradientn(colours = c("darkblue", "red", "orange",  "yellow")) +
theme_hn() -> tile_plot
)
# ask und best etc. rausfiltern
# Plot wie sich einzelne Storys entwickeln in diesem Raster
# normalisieren
# return data frame with IDs of new stories during scraping
get_id_selector <- function(data) {
data %>%
# TODO: DECIDE WHAT TO DO ABOUT STORIES THAT WERE ALREADY THERE
# dplyr::filter(age < 120) %>%
select(id) %>%
unique() -> id_selector
return(id_selector)
}
# subset the data by a random sample of IDs
get_random_sample <- function(data, n) {
id_selector <- get_id_selector(data)
if (dim(id_selector)[1] < n) {
stop("n is too large")
}
data %>% inner_join(sample_n(id_selector, n), by = "id") -> random_sample
return(random_sample)
}
# subset the data by the first n IDs that occurred
get_first_n <- function(data, n) {
id_selector <- get_id_selector(data)
if (dim(id_selector)[1] < n) {
stop("n is too large")
}
first_n_sample <- data %>% inner_join(head(id_selector, n), by = "id")
return(first_n_sample)
}
sampling_start <- min(data$samptime_datetime)
sampling_end <- max(data$samptime_datetime)
cutof_start <- lubridate::ceiling_date(sampling_start, "day")
cutof_end <- lubridate::floor_date(sampling_end, "day")
data %>%
filter((samptime_datetime >= cutof_start) & (samptime_datetime <= cutof_end)) -> data_cropped
data %>%
ggplot(aes(x = subtime_datetime)) +
geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
labs(title = "KDE of Posting Volume",
x = "Submission Time",
y = "KDE",
caption = "The red dashed line indicates the start of the sampling.") +
geom_vline(xintercept = min(data$samptime_datetime), size = 1, linetype = "dashed", color = "red") +
theme_hn() +
theme(axis.text.y = element_blank())
data_cropped %>%
ggplot(aes(x = subtime_datetime)) +
geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
labs(title = "KDE of Posting Volume",
x = "Submission Time",
y = "KDE",
caption = "The red dashed line indicates the start of the sampling.") +
geom_vline(xintercept = min(data$samptime_datetime), size = 1, linetype = "dashed", color = "red") +
theme_hn() +
theme(axis.text.y = element_blank())
sampling_start <- min(data$samptime_datetime)
sampling_end <- max(data$samptime_datetime)
cutof_start <- lubridate::ceiling_date(sampling_start, "day")
cutof_end <- lubridate::floor_date(sampling_end, "day")
data %>%
filter((subtime_datetime >= cutof_start) & (subtime_datetime <= cutof_end)) -> data_cropped
data_cropped %>%
ggplot(aes(x = subtime_datetime)) +
geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
labs(title = "KDE of Posting Volume",
x = "Submission Time",
y = "KDE",
caption = "The red dashed line indicates the start of the sampling.") +
geom_vline(xintercept = min(data$samptime_datetime), size = 1, linetype = "dashed", color = "red") +
theme_hn() +
theme(axis.text.y = element_blank())
data_cropped %>%
ggplot(aes(x = subtime_clocktime)) +
geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
labs(title = "KDE of Posting Volume by Time of Day",
x = "Time of day",
y = "KDE",
caption = "The displayed times are in CET.") +
scale_x_datetime(date_labels = "%H:%M:%S") +
theme_hn() +
theme(axis.text.y = element_blank())
lubridate::with_tz("2021-01-01 19:00:00", tzone = "America/Los_Angeles")
if (!("data-posting-volume.feather" %in% list.files(file.path("..", "data")))) {
data_cropped %>%
mutate(subtime_date = lubridate::as_date(subtime_datetime),
subtime_hour = lubridate::hour(subtime_datetime)) %>%
select(id, subtime_date, subtime_hour) %>%
distinct() %>%
group_by(subtime_date, subtime_hour) %>%
summarize(count = n()) %>%
ungroup() %>%
group_by(subtime_hour) %>%
summarize(mean_count_per_hour = mean(count)) %>%
ungroup() %>%
mutate(mean_count_per_hour_standardized = (mean_count_per_hour - mean(mean_count_per_hour)) / sd(mean_count_per_hour),
subtime_hour_scaled = subtime_hour / 24) %>%
select(subtime_hour, subtime_hour_scaled, mean_count_per_hour, mean_count_per_hour_standardized) -> data_posting_volume
arrow::write_feather(data_posting_volume, sink = file.path("..", "data", "data-posting-volume.feather"))
} else {
data_posting_volume <- arrow::read_feather(file.path("..", "data", "data-posting-volume.feather"))
}
if (!("data-posting-volume.feather" %in% list.files(file.path("..", "data")))) {
data_cropped %>%
mutate(subtime_date = lubridate::as_date(subtime_datetime),
subtime_hour = lubridate::hour(subtime_datetime)) %>%
select(id, subtime_date, subtime_hour) %>%
distinct() %>%
group_by(subtime_date, subtime_hour) %>%
summarize(count = n()) %>%
ungroup() %>%
group_by(subtime_hour) %>%
summarize(mean_count_per_hour = mean(count)) %>%
ungroup() %>%
mutate(mean_count_per_hour_standardized = (mean_count_per_hour - mean(mean_count_per_hour)) / sd(mean_count_per_hour),
subtime_hour_scaled = subtime_hour / 24) %>%
select(subtime_hour, subtime_hour_scaled, mean_count_per_hour, mean_count_per_hour_standardized) -> data_posting_volume
arrow::write_feather(data_posting_volume, sink = file.path("..", "data", "data-posting-volume.feather"))
} else {
data_posting_volume <- arrow::read_feather(file.path("..", "data", "data-posting-volume.feather"))
}
data_posting_volume %>%
ggplot(aes(x = subtime_hour, y = mean_count_per_hour)) +
geom_point(size = 2) +
ylim(c(10, 80)) +
scale_x_continuous(breaks = seq(0, 23, by = 6)) +
labs(title = "Average Posting Volume per Hour",
x = "Hour of Day",
y = "Average Number of New Stories",
caption = "The red dashed line indicates the daily average.") +
geom_hline(yintercept = mean(data_posting_volume$mean_count_per_hour),
linetype = "dashed", color = "red", size = 1) +
theme_hn()
data_posting_volume %>%
ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
geom_point() +
ylim(c(-2, 2)) +
labs(title = "Average Posting Volume per Hour (Scaled)",
x = "Time of Day [0, 1]",
y = "Average Number of New Stories [Z-Scores]") +
geom_hline(yintercept = 0, linetype = "solid", size = 1) +
theme_hn()
adj_sin <- function(x) {
-sin(2 * pi * x)
}
adj_sin2 <- function(x) {
1.5 * (-sin(2 * pi * x))
}
data_posting_volume %>%
ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
geom_point() +
xlim(c(0, 1)) +
ylim(c(-2, 2)) +
labs(title = "Average Posting Volume per Hour (Scaled)",
x = "Time of Day [0, 1]",
y = "Average Number of New Stories [Z-Scores]") +
geom_function(fun = adj_sin, color = "blue", size = 1) +
geom_function(fun = adj_sin2, color = "red", size = 1) +
geom_hline(yintercept = 0, linetype = "solid", size = 1) +
theme_hn()
param <- 1.3441739239406807
adj_sin <- function(x, alpha=1.3441739239406807) {
-alpha * sin(2 * pi * x)
}
data_posting_volume %>%
ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
geom_point() +
ylim(c(-2, 2)) +
xlim(c(0, 1)) +
labs(title = "Average Posting Volume per Hour (Scaled)",
x = "Time of Day [0, 1]",
y = "Average Number of New Stories [Z-Scores]",
caption = glue::glue("The red line is the fitted function with alpha = {round(param, 3)}")) +
geom_function(fun = adj_sin, color = "firebrick", size = 1) +
geom_hline(yintercept = 0, linetype = "solid", size = 1) +
theme_hn()
sampling_start <- min(data$samptime_datetime)
sampling_end <- max(data$samptime_datetime)
cutof_start <- lubridate::ceiling_date(sampling_start, "day")
cutof_end <- lubridate::floor_date(sampling_end, "day")
data %>%
filter((subtime_datetime >= cutof_start) & (subtime_datetime <= cutof_end)) -> data_subtimes_cropped
# get IDs and submission times and sort them by temporally
data_subtimes_cropped %>%
select(id, subtime_datetime) %>%
distinct() %>%
arrange(subtime_datetime) %>%
select(subtime_datetime) %>%
mutate(subtime_clocktime = update(subtime_datetime, yday = 1)) -> sorted_submission_times
sorted_submission_times
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
# simulate this process
set.seed(1)
n <- dim(arrival_counts)[1]
lambda <- mean(arrival_counts$arr_count)
simulated_distribution <- tibble(simulated_arr_count = rpois(n, lambda))
arrival_counts_long <- arrival_counts %>%
cbind(simulated_distribution) %>%
tidyr::pivot_longer(cols = c(arr_count, simulated_arr_count), names_to = "type", values_to = "value")
arrival_counts_long$type %<>% factor()
levels(arrival_counts_long$type) <- list(empirical = "arr_count", simulated = "simulated_arr_count")
# plot generics
y_max <- 6000
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
# plot generics
y_max <- 10000
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
arrival_counts %>%
mutate(minute_of_day = as.numeric(bin) %% 1440) %>%
group_by(minute_of_day) %>%
summarize(mean_arr_count = mean(arr_count)) %>%
ungroup() -> mean_arrival_count_per_minute
mean_arrival_count_per_minute$mean_arr_count %>% sd()
simulated_arrivals <- arrow::read_feather(file.path("..", "data", "simulated_arrivals.feather"))
simulated_arrivals %>%
group_by(hour_of_day) %>%
summarize(mean_arr_count = sum(arr_count)) %>%
ungroup() %>%
ggplot(aes(x = hour_of_day, y = mean_arr_count)) +
geom_point()
simulated_arrivals %>%
ggplot(aes(x = time_index, y = arr_count)) +
geom_point(alpha = 0.3)
data_posting_volume %>%
rename(hour_of_day = subtime_hour) %>%
inner_join(simulated_arrivals %>%
group_by(hour_of_day) %>%
summarize(sum_sim_arr_count = sum(arr_count)) %>%
ungroup(),
by = "hour_of_day") %>%
select(hour_of_day, mean_count_per_hour, sum_sim_arr_count) -> emp_vs_sim
emp_vs_sim %>%
tidyr::pivot_longer(cols = c(mean_count_per_hour, sum_sim_arr_count), names_to = "type", values_to = "value") %>%
ggplot(aes(x = hour_of_day, y = value, color = type)) +
geom_point() +
scale_color_manual(values = viridis::cividis(2)) +
theme_hn()
(
data_new %>%
filter(rank_newpage < 100) %>%
filter(rank_toppage < 60) %>%
filter(count > 10) %>%
mutate(gained_mean = log(gained_mean)) %>%
# replace_na(list(gained = 0)) %>%
mutate_all(function(x) ifelse(is.infinite(x), -7, x)) %>%
ggplot(aes(x = rank_newpage, y = rank_toppage, fill = gained_mean)) +
geom_tile() +
labs(title = "Gained votes on new vs. top page",
x = "newpage rank",
y = "toppage rank",
fill = "gained votes") +
scale_y_reverse() +
coord_fixed() +
scale_fill_viridis_c() +
# scale_fill_gradient(low = "darkblue", high = "yellow") +
# scale_fill_gradientn(colours = c("darkblue", "red", "orange",  "yellow")) +
theme_hn() -> tile_plot
)
# ask und best etc. rausfiltern
# Plot wie sich einzelne Storys entwickeln in diesem Raster
# normalisieren
0.8 ^3
3 * 0.2^2 * 0.8^1 + 2 * 0.2^1 * 0.8^2 + 1 * 0.2^0 * 0.8^3
3 * 0.2^2 * 0.8^1 + 3 * 0.2^1 * 0.8^2 + 1 * 0.2^0 * 0.8^3
# defaults and libraries
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
library(magrittr)
library(tidyr)
library(gganimate)
source("helpers.R")
# load / preprocess data
if (!("hn-data-processed.rds" %in% list.files(file.path("..", "data")))) {
source("preprocess.R")
} else {
data <- readr::read_rds(file.path("..", "data", "hn-data-processed.rds"))
}
# First, we need to extract all the submission times and sort them chronologically.
sampling_start <- min(data$samptime_datetime)
data %>% filter((subtime_datetime >= sampling_start)) -> data_subtimes_cropped
data_subtimes_cropped
# First, we need to extract all the submission times and sort them chronologically.
sampling_start <- min(data$samptime_datetime)
data %>% filter((subtime_datetime >= sampling_start)) -> data_subtimes_cropped
# crop the data to only include stories which were surveyed from the time of their submission
sampling_start <- min(data$samptime_datetime)
data %>% filter((subtime_datetime >= sampling_start)) -> data_subtimes_cropped
# get IDs and submission times and sort them in ascending order by submission time
data_subtimes_cropped %>%
select(id, subtime_datetime) %>%
distinct() %>%
arrange(subtime_datetime) %>%
select(subtime_datetime) %>%
mutate(subtime_clocktime = update(subtime_datetime, yday = 1)) -> sorted_submission_times
# COMMENT: All submission dates are set to the first of January.
#          This is necessary to obtain the clock time because
#          there is no convenient clock time format. This will
#          work though.
sorted_submission_times
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_clocktime,
bin = cut(sorted_submission_times$subtime_clocktime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
arrival_counts
# simulate this process
set.seed(1)
n <- dim(arrival_counts)[1]
lambda <- mean(arrival_counts$arr_count)
simulated_distribution <- tibble(simulated_arr_count = rpois(n, lambda))
arrival_counts_long <- arrival_counts %>%
cbind(simulated_distribution) %>%
tidyr::pivot_longer(cols = c(arr_count, simulated_arr_count), names_to = "type", values_to = "value")
arrival_counts_long$type %<>% factor()
levels(arrival_counts_long$type) <- list(empirical = "arr_count", simulated = "simulated_arr_count")
# plot generics
y_max <- 10000
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
# plot generics
y_max <- 100
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
# plot generics
y_max <- 1000
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
arrival_counts
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE))
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
arrival_counts
# simulate this process
set.seed(1)
n <- dim(arrival_counts)[1]
lambda <- mean(arrival_counts$arr_count)
simulated_distribution <- tibble(simulated_arr_count = rpois(n, lambda))
arrival_counts_long <- arrival_counts %>%
cbind(simulated_distribution) %>%
tidyr::pivot_longer(cols = c(arr_count, simulated_arr_count), names_to = "type", values_to = "value")
arrival_counts_long$type %<>% factor()
levels(arrival_counts_long$type) <- list(empirical = "arr_count", simulated = "simulated_arr_count")
# plot generics
y_max <- 10000
rate_max <- 7
arrival_counts_long %>%
ggplot(aes(x = value, fill = type)) +
geom_bar(position = "dodge", alpha = 0.6, color = "transparent") +
ylim(c(0, y_max)) +
scale_x_continuous(trans = "reverse", breaks = seq(rate_max, 0, by = -1), limits = c(rate_max + 0.5, -0.5)) +
scale_fill_manual(values = c("empirical" = "black", "simulated" = "red")) +
labs(title = "Distribution of Arrival Rate per Minute",
x = "Arrival Rate",
y = "Count") +
coord_flip() +
theme_hn() +
theme(panel.grid.minor.x = element_blank(),
panel.grid.minor.y = element_blank(),
legend.title = element_blank())
# get IDs and submission times and sort them in ascending order by submission time
data_subtimes_cropped %>%
select(id, subtime_datetime) %>%
distinct() %>%
arrange(subtime_datetime) %>%
select(subtime_datetime) -> sorted_submission_times
sorted_submission_times
# crop the data to only include stories which were surveyed from the time of their submission
sampling_start <- min(data$samptime_datetime)
data %>% filter((subtime_datetime >= sampling_start)) -> data_subtimes_cropped
# get IDs and submission times and sort them in ascending order by submission time
data_subtimes_cropped %>%
select(id, subtime_datetime) %>%
distinct() %>%
arrange(subtime_datetime) %>%
select(subtime_datetime) -> sorted_submission_times
# COMMENT: All submission dates are set to the first of January.
#          This is necessary to obtain the clock time because
#          there is no convenient clock time format. This will
#          work though.
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
# crop the data to only include stories which were surveyed from the time of their submission
sampling_start <- min(data$samptime_datetime)
data %>% filter((subtime_datetime >= sampling_start)) -> data_subtimes_cropped
# get IDs and submission times and sort them in ascending order by submission time
data_subtimes_cropped %>%
select(id, subtime_datetime) %>%
distinct() %>%
arrange(subtime_datetime) %>%
select(subtime_datetime) -> sorted_submission_times
# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime,
bin = cut(sorted_submission_times$subtime_datetime,
breaks = "1 min", labels = FALSE)) %>%
mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>%
group_by(bin, .drop = FALSE) %>%
summarize(arr_count = n()) -> arrival_counts
arrival_counts
