---
title: "Hacker News Analysis"
output: 
  prettydoc::html_pretty:
    theme: tactile
---

```{r setup, include=FALSE, warning=FALSE}

# defaults and libraries
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
library(magrittr)
library(gganimate)
source("helpers.R")

# load / preprocess data
if (!("hn-data-processed.rds" %in% list.files(file.path("..", "data")))) {
  source("preprocess.R")
} else {
  data <- readr::read_rds(file.path("..", "data", "hn-data-processed.rds"))  
}

```





```{r helper-functions}

# return data frame with IDs of new stories during scraping
get_id_selector <- function(data) {
  data %>% 
    # TODO: DECIDE WHAT TO DO ABOUT STORIES THAT WERE ALREADY THERE
    # dplyr::filter(age < 120) %>% 
    select(id) %>% 
    unique() -> id_selector  
  return(id_selector)
}

# subset the data by a random sample of IDs
get_random_sample <- function(data, n) {
  id_selector <- get_id_selector(data)
  if (dim(id_selector)[1] < n) {
    stop("n is too large")
  }
  data %>% inner_join(sample_n(id_selector, n), by = "id") -> random_sample
  return(random_sample)
}

# subset the data by the first n IDs that occurred 
get_first_n <- function(data, n) {
  id_selector <- get_id_selector(data)
  if (dim(id_selector)[1] < n) {
    stop("n is too large")
  }
  first_n_sample <- data %>% inner_join(head(id_selector, n), by = "id")  
  return(first_n_sample)
}

```


```{r cold-start-robust-data}

sampling_start <- min(data$samptime_datetime)
sampling_end <- max(data$samptime_datetime)

cutof_start <- lubridate::ceiling_date(sampling_start, "day")
cutof_end <- lubridate::floor_date(sampling_end, "day")

data %>% 
  filter((samptime_datetime >= cutof_start) & (samptime_datetime <= cutof_end)) -> data_cropped

```






# Distribution of Submissions over Time

Posting activities seem to fluctuate in a regular pattern. The period for this fluctuation pattern is very apparent: It's daily. The following plot shows a kernel density estimation plotted over time. I included the red dashed line because the scraper has some sort of a cold start problem: The prior development of already existing stories at the time of starting the scraper are not in the data. This may lead to some distortions which is why I will probably exclude the stories for which this is the case.

```{r kde-posting-volume-datetime}

data %>% 
  ggplot(aes(x = subtime_datetime)) +
  geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
  labs(title = "KDE of Posting Volume",
       x = "Submission Time",
       y = "KDE",
       caption = "The red dashed line indicates the start of the sampling.") +
  geom_vline(xintercept = min(data$samptime_datetime), size = 1, linetype = "dashed", color = "red") +
  theme_hn() +
  theme(axis.text.y = element_blank())

```

As already mentioned, the activity seems to fluctuate on a daily basis. It makes sense to aggregate over all days and look into what a typical day on Hacker News might look like. 
For this analysis, we look at a subset of the data set that accounts for different start and end times of the scraper. We cropped to the ceiling of the first sampling day and the floor of the last one. 


```{r kde-posting-volume-time}

data_cropped %>% 
  ggplot(aes(x = subtime_clocktime)) +
  geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
  labs(title = "KDE of Posting Volume by Time of Day",
       x = "Time of day",
       y = "KDE",
       caption = "The displayed times are in CET.") +
  scale_x_datetime(date_labels = "%H:%M:%S") +
  theme_hn() +
  theme(axis.text.y = element_blank())

```
We can see that (from the perspective of Central European Time) there is low traffic in the early morning, then it picks up and increases until it reaches its peak at around 6 or 7 pm CET. 

```{r}

lubridate::with_tz("2021-01-01 19:00:00", tzone = "America/Los_Angeles")

```

At 7 pm CET, it is 10 am PST. Going out on a limb, I would argue that the peak is probably due to work days starting in Silicon Valley, as presumably, a significant portion of the user base is located there.  




The following analysis will clarify the substructure of this posting volume fluctuation. 

```{r}
data_cropped %>% 
  mutate(subtime_date = lubridate::as_date(subtime_datetime),
         subtime_hour = lubridate::hour(subtime_datetime)) %>% 
  select(id, subtime_date, subtime_hour) %>% 
  distinct() %>% 
  group_by(subtime_date, subtime_hour) %>% 
  summarize(count = n()) %>% 
  ungroup() %>%
  group_by(subtime_hour) %>% 
  summarize(mean_count_per_hour = mean(count)) %>% 
  ungroup() %>% 
  mutate(mean_count_per_hour_standardized = (mean_count_per_hour - mean(mean_count_per_hour)) / sd(mean_count_per_hour),
         subtime_hour_scaled = subtime_hour / 24) %>% 
  select(subtime_hour, subtime_hour_scaled, mean_count_per_hour, mean_count_per_hour_standardized) -> data_posting_volume

```

```{r}
if (!("data-posting-volume.feather" %in% list.files(file.path("..", "data")))) {
  arrow::write_feather(data_posting_volume, sink = file.path("..", "data", "data-posting-volume.feather"))
}
```


Let's first look at the posting volume aggregated and summarized for each hour of the day. The following plot shows the mean number of posts for each hour of the day. Arguably, this is an "average" day on Hacker News. 

```{r}
data_posting_volume %>% 
  ggplot(aes(x = subtime_hour, y = mean_count_per_hour)) +
  geom_point(size = 2) +
  ylim(c(10, 80)) +
  scale_x_continuous(breaks = seq(0, 23, by = 6)) +
  labs(title = "Average Posting Volume per Hour",
       x = "Hour of Day",
       y = "Average Number of New Stories",
       caption = "The red dashed line indicates the daily average.") +
  geom_hline(yintercept = mean(data_posting_volume$mean_count_per_hour), 
             linetype = "dashed", color = "red", size = 1) +
  theme_hn()
```

If we transform this data a little bit, we might be able to fit a simple function to it, so that we can simulate the fluctuations of the arrival rate over the day. 

```{r}
data_posting_volume %>% 
  ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
  geom_point() +
  ylim(c(-2, 2)) +
  labs(title = "Average Posting Volume per Hour (Scaled)",
       x = "Time of Day [0, 1]",
       y = "Average Number of New Stories [Z-Scores]") +
  geom_hline(yintercept = 0, linetype = "solid", size = 1) +
  theme_hn()
```

The hour of the day was scaled to the interval [0, 1] and the average number of new stories per hour was standardized (i.e., Z-scores were computed). Eye-balling it, I would say that this trend curve can be approximated pretty well with the sine function. Strikingly, the saddle point seems to be exactly in the middle of the day, making it even more convenient to fit a sine function to the data. 


```{r}


adj_sin <- function(x) {
  -sin(6.4 * x)
}
adj_sin2 <- function(x) {
  1.5 * (-sin(6.4 * x))
}

data_posting_volume %>% 
  ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
  geom_point() +
  ylim(c(-2, 2)) +
  labs(title = "Average Posting Volume per Hour (Scaled)",
       x = "Time of Day [0, 1]",
       y = "Average Number of New Stories [Z-Scores]") +
  geom_function(fun = adj_sin, color = "blue", size = 1) +
  geom_function(fun = adj_sin2, color = "red", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", size = 1) +
  theme_hn()
```
This plot shows the data overlaid with the functions $-sin(\beta x)$ and $-\alpha sin(\beta x)$ with "hand-adjusted $\alpha$ and $\beta$. Obviously, the two parameters can be fitted with a simple least squared approach, but so far, I didn't have time to implement such an approach (especially not in R...).




```{r}

adj_sin <- function(x, alpha=6.17725586778391) {
  -sin(alpha * x)
}

data_posting_volume %>% 
  ggplot(aes(x = subtime_hour_scaled, y = mean_count_per_hour_standardized)) +
  geom_point() +
  ylim(c(-2, 2)) +
  xlim(c(0, 1)) +
  labs(title = "Average Posting Volume per Hour (Scaled)",
       x = "Time of Day [0, 1]",
       y = "Average Number of New Stories [Z-Scores]") +
  geom_function(fun = adj_sin, color = "firebrick", size = 1) +
  geom_hline(yintercept = 0, linetype = "solid", size = 1) +
  theme_hn()

```


<!--- TO DO: REFACTOR THIS PART --->
<!--- \START --->

Another approach to get insight into the arrival rates of new stories on Hacker News is to look into the actual rates of arrival which our data affords. I will operate with the hypothesis that this data is Poisson distributed. Rates of arrival are the archetypal case for the Poisson distribution, so I suppose it is reasonable to assume this. 


```{r format-data-for-submission-times}
# get IDs and submission times and sort them by temporally
data %>% 
  select(id, subtime_datetime) %>% 
  distinct() %>% 
  arrange(subtime_datetime) %>% 
  select(subtime_datetime) -> sorted_submission_times

# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime, 
       bin = cut(sorted_submission_times$subtime_datetime, 
                 breaks = "1 min", labels = FALSE)) %>% 
  mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>% 
  group_by(bin, .drop = FALSE) %>% 
  summarize(arr_count = n()) -> arrival_counts

```




```{r plot-submission-times-distributions}
# generics
y_max <- 7000
rate_max <- 7

# show real distribution of arrival times
p1 <- arrival_counts %>% 
  ggplot(aes(x = arr_count)) +
  geom_bar(color = "transparent", fill = "black", alpha = 0.8) +
  ylim(c(0, y_max)) +
  scale_x_continuous(breaks = seq(0, 7, by = 1), limits = c(-0.5, rate_max + 0.5)) +
  labs(title = "Distribution of Arrival Rate per Minute (Empirical)",
       x = "Arrival Rate",
       y = "Count") +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

# simulate this process
set.seed(1)
n <- dim(arrival_counts)[1]
lambda <- mean(arrival_counts$arr_count)
simulated_distribution <- tibble(arr_count = rpois(n, lambda))

# show simulated distribution of arrival times
p2 <- simulated_distribution %>% 
  ggplot(aes(x = arr_count)) +
  geom_bar(color = "transparent", fill = "black", alpha = 0.8) +
  ylim(c(0, y_max)) +
  scale_x_continuous(breaks = seq(0, 7, by = 1), limits = c(-0.5, rate_max + 0.5)) +
  labs(title = "Distribution of Arrival Rate per Minute (Simulated)",
       x = "Arrival Rate",
       y = "Count")  +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

ggpubr::ggarrange(p1, p2, ncol = 1)

```

As can be seen, arrival rates can be simulated with this fitted Poisson process very accurately. If we can determine the user behavior equally well, we can find a good mapping between real-world time and simulation time.


lambda -> "sinus(t)"

<!--- \END --->


























```{r}
data %>% 
  select(id, subtime_clocktime) %>% 
  distinct() %>% 
  arrange(subtime_clocktime) %>% 
  group_by(subtime_clocktime) %>% 
  summarize(count = n()) %>% 
  ungroup() %>% 
  arrange(subtime_clocktime) -> subs_per_minute
```




```{r}


tmp %>% 
  ggplot(aes(x = subtime_clocktime, y = count)) +
  geom_point()





mean_count <- mean(tmp2$count)

adj_sin <- function(x) {
  1.5 * sin(-6.4 * x)
}
adj_sin2 <- function(x) {
  sin(-6.4 * x)
}



tmp2 %>% 
  ggplot(aes(x = subtime_hour_slice, y = count_standardized)) +
  geom_point() +
  ylim(c(-2, 2)) +
  geom_hline(yintercept = 0, linetype = "dashed") +
  geom_function(fun = adj_sin, color = "red", size = 2) +
  geom_function(fun = adj_sin2, color = "green", size = 2) +
  geom_smooth() +
  theme_hn()




tmp2 %>% 
  mutate(count_standardized_pred = adj_sin(subtime_hour_slice)) %>% 
  mutate(error = abs(count_standardized - count_standardized_pred)) -> tmp3



mse <- mean((tmp3$error)^2)
rmse <- sqrt(mse)

mse
rmse

```

```{r}
simulated_arrivals <- arrow::read_feather(file.path("..", "data", "simulated_arrivals3.feather"))
simulated_arrivals %<>% mutate(count_simulated = simulated_arrivals) %>% select(-simulated_arrivals)
```


```{r}
simulated_arrivals %>% 
  inner_join(subcounts_real, by = "time_index") %>% 
  select(time_index, count_simulated, subtime_hour) %>% 
  group_by(subtime_hour) %>% 
  summarize(count = sum(count_simulated)) %>% 
  ungroup() %>% 
  ggplot(aes(x = subtime_hour, y = count)) +
  geom_point()
```

This approach doesn't seem to work yet. We have to find some way of estimating the function that produces lambda for the Poisson distribution at any given time. 


```{r}
# transform real submission counts
start <- lubridate::as_datetime("2021-01-01 00:00:00 CET") %>% as.numeric()
minute_slices <- seq(0, (60 * 60 * 24) - 1, by = 60)
norm_value <- rep(start, 60 * 24)
df <- data.frame(x = minute_slices, y = norm_value)
df %<>% 
  mutate(unix_dt = y + x) %>% 
  mutate(subtime_coarse = lubridate::as_datetime(unix_dt)) %>% 
  mutate(time_index = row_number())
df %>% 
  left_join(tmp, by = "subtime_coarse") %>% 
  tidyr::replace_na(replace = list(count = 0)) %>% 
  mutate(subtime_hour = lubridate::hour(subtime_coarse)) %>% 
  mutate(count_real = count) %>% 
  select(time_index, subtime_coarse, subtime_hour, count_real) -> subcounts_real
  
# compare to simulated submission counts
subcounts_real %>% 
  inner_join(simulated_arrivals, by = "time_index") %>% 
  # group_by(subtime_hour) %>% 
  # summarize(count_real = sum(count_real),
  #           count_simulated = sum(count_simulated)) %>% 
  # ungroup()
  identity()

```





```{r}

# TODO: CREATE SHINY WIDGET TO DISPLAY DIFFERENT TIME ZONES
  
# mutate(subtime_clocktime = update(subtime_datetime, yday = 1))

```








# How do stories develop on the `New` page?


<!--- START -> THIS IS WHAT YOU ARE WORKING ON --->

First of all, let's look into how many votes are cast on the top page as opposed to not on the top page


```{r}
# COPIED FROM BELOW
data %>% 
  select(id, age_seconds, age_hours, score, rank_toppage, subtime_datetime, samptime_datetime) %>%
  mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  group_by(samptime_coarse) %>%
  arrange(desc(subtime_datetime)) %>% 
  mutate(rank_newpage = row_number()) %>% 
  ungroup() %>% 
  arrange(samptime_coarse) %>% 
  mutate(on_toppage = !is.na(rank_toppage)) -> newpage_moving_window

# COMMENT: alternatively, the following function can be used to observe only the first 200 elements on the new page
# slice_max(n = 200, order_by = subtime_datetime)
```


```{r}
newpage_moving_window %>% 
  select(id, score, rank_toppage, rank_newpage, samptime_datetime) %>% 
  mutate(on_toppage = !(is.na(rank_toppage))) %>% 
  arrange(id, samptime_datetime) -> data_gained_votes

data_gained_votes %>% 
  group_by(id) %>% 
  mutate(next_score = lead(score)) %>% 
  ungroup() %>% 
  mutate(gained_votes = next_score - score) %>% 
  filter(!is.na(gained_votes)) -> gained_votes
```


```{r}
gained_votes %>% 
  group_by(on_toppage) %>% 
  summarize(count = sum(gained_votes)) %>% 
  ungroup()
```



```{r}

gained_votes %>% 
  group_by(rank_toppage) %>% 
  summarize(mean_gained_votes = mean(gained_votes)) %>% 
  ungroup() %>% 
  ggplot(aes(x = rank_toppage, y = mean_gained_votes)) +
  geom_line()

```

Erklärungsansätze: Warum auch auf hinteren Rängen noch Votes?
* Zeitversatz?



Reasonable assumption: The distribution of samples for each rank is about uniformly distributed

```{r}
gained_votes %>% 
  # filter(rank_toppage <= 100) %>% 
  ggplot(aes(x = rank_toppage)) +
  geom_bar()
```

```{r}
# compute number of votes cast at each rank
gained_votes %>% 
  group_by(rank_toppage) %>% 
  summarize(gained_votes_sum = sum(gained_votes)) %>% 
  ungroup() -> tmp

# format data for distribution fitting
tmp_matrix <- as.matrix(tmp)
x <- c()
for (i in 1:length(tmp_matrix[, 1])) {
  values <- rep(tmp_matrix[i, 1], tmp_matrix[i, 2])
  x <- c(x, values)
}
x %<>% unname()
x <- x[!is.na(x)]

# show distribution over first 100 ranks on top page
tibble(vote_at_rank = x) %>% 
  filter(vote_at_rank <= 100) %>% 
  ggplot(aes(x = vote_at_rank)) +
  geom_bar() +
  geom_vline(xintercept = 30, linetype = "dashed", color = "red")

```

```{r}
# fit exponential distribution
exp_estimator <- MASS::fitdistr(x, densfun = "exponential")
x_sim <- rexp(length(x), rate = exp_estimator$estimate)
x_sim %<>% round()
tibble(simulated_vote_at_rank = x_sim) %>% 
  filter(simulated_vote_at_rank <= 100) %>% 
  ggplot(aes(x = simulated_vote_at_rank)) +
  geom_bar()

```

```{r}

# fit geometric distribution
geom_estimator <- MASS::fitdistr(x, densfun = "geometric")
x_sim <- rgeom(length(x), geom_estimator$estimate)
x_sim <- x_sim + 1
tibble(simulated_vote_at_rank = x_sim) %>% 
  filter(simulated_vote_at_rank <= 100) %>% 
  ggplot(aes(x = simulated_vote_at_rank)) +
  geom_bar()

```
Pareto Verteilung?

Zipf distribution? -> very likely -> !

<!--- END -> THIS IS WHAT YOU ARE WORKING ON --->












```{r}

# TODO: INVESTIGATE OUTLIERS

gained_votes %>% 
  group_by(rank_newpage) %>% 
  summarize(mean_gained_votes = mean(gained_votes)) %>% 
  ungroup() %>% 
  ggplot(aes(x = rank_newpage, y = mean_gained_votes)) +
  geom_point(size = 0.01)

```

```{r}

# TODO: INVESTIGATE OUTLIERS

gained_votes %>% 
  # group_by(rank_newpage) %>% 
  # summarize(mean_gained_votes = mean(gained_votes)) %>% 
  # ungroup() %>% 
  ggplot(aes(x = rank_newpage, y = gained_votes)) +
  geom_point(size = 0.01)

```







Developments on the new page

```{r}
data %>% 
  select(id, age_seconds, age_hours, score, rank_toppage, subtime_datetime, samptime_datetime) %>%
  mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  group_by(samptime_coarse) %>%
  arrange(desc(subtime_datetime)) %>% 
  mutate(rank_newpage = row_number()) %>% 
  ungroup() %>% 
  arrange(samptime_coarse) %>% 
  mutate(on_toppage = !is.na(rank_toppage)) -> newpage_moving_window

# COMMENT: alternatively, the following function can be used to observe only the first 200 elements on the new page
# slice_max(n = 200, order_by = subtime_datetime)
```


The next plot shows how the scores of stories develops while they are on the new page.

```{r, eval=FALSE}
color_scheme <- c("FALSE" = "darkgrey", "TRUE" = "red")
set.seed(123)
newpage_moving_window %>% 
  filter(rank_newpage <= 1000) %>% 
  get_random_sample(50) %>% 
  ggplot(aes(x = rank_newpage, y = score, group = id, color = on_toppage)) +
  geom_jitter(alpha = 0.1, height = 0.05) +
  scale_x_continuous(trans = "reverse") +
  scale_color_manual(values = color_scheme) +
  scale_y_log10() +
  labs(title = "How do stories emerge from the new page to the top page?",
       x = "Newpage Rank",
       y = "Score", 
       color = "On Toppage") +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())
```

```{r}

color_scheme <- c("FALSE" = "darkgrey", "TRUE" = "red")
set.seed(123)
newpage_moving_window %>%
  sample_n(20000) %>% 
  filter(score <= 1000) %>% 
  # get_random_sample(50) %>% 
  ggplot(aes(x = age_hours, y = score, color = on_toppage)) +
  # geom_jitter(alpha = 0.1, height = 0.05) +
  geom_point(alpha = 0.2) +
  # scale_x_continuous(trans = "reverse") +
  scale_color_manual(values = color_scheme) +
  # scale_y_log10() +
  labs(title = "How do stories emerge from the new page to the top page?",
       x = "Newpage Rank",
       y = "Score", 
       color = "On Toppage") +
  # coord_flip() +
  facet_wrap(. ~ on_toppage) +
  theme_hn() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

```


Annahme: erste drei Votes sind random
Qualität und Clickbaitiness



```{r}

color_scheme <- c("FALSE" = "darkgrey", "TRUE" = "red")

set.seed(123)

newpage_moving_window %>% 
  get_random_sample(100) %>% 
  filter(age_seconds <= 50000) %>%
  ggplot(aes(x = age_hours, y = score, group = id, color = on_toppage)) +
  geom_jitter(alpha = 0.05, height = 0.01) +
  scale_color_manual(values = color_scheme) +
  # scale_y_log10() +
  theme_hn() +
  NULL
```

```{r}

# TODO: COMPUTE HOW MANY VOTES STORIES GET WHEN THEY ARE NOT ON THE TOP PAGE

newpage_moving_window %>% 
  group_by(id) %>% 
  slice(if(any(on_toppage == FALSE)) 1:which.max(on_toppage == FALSE) else row_number())
  
  
```















How many votes do stories gain while on the `new` page?

```{r}

newpage_moving_window %>% 
  group_by(id) %>% 
  summarize(gained_votes = max(score) - 1) %>% 
  ungroup() -> gained_votes_per_id_newpage

gained_votes_per_id_newpage %>% 
  filter(!is.na(gained_votes)) -> gained_votes_per_id_newpage_nondeleted

gained_votes_per_id_newpage_nondeleted$gained_votes %>% mean()


gained_votes_per_id_newpage %>% 
  filter(!is.na(gained_votes)) %>% 
  group_by(gained_votes) %>% 
  summarize(count = n()) %>% 
  filter(gained_votes <= 100) %>% 
  ggplot(aes(x = gained_votes, y = count)) +
  geom_bar(stat = "identity")

```

Each story seems to gain 17.4 votes while on the `new` page on average. Whether or not that is due to being there is not clear. This has to be contrasted with what happens when the stories goes further down on the `new` page.


```{r}

data %>% 
  filter(id == 25870990)

```

Some stories seem to be deleted after some time, resulting in a score NA (see table above).








# Do stories get upvotes if they pass the `New` page without getting any upvotes?


```{r}
data %>% 
  dplyr::select(id, score, rank_toppage, subtime_datetime, samptime_datetime) %>%
  # dplyr::rename(rank_toppage = rank) %>% 
  dplyr::mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  dplyr::group_by(samptime_coarse) %>%
  dplyr::arrange(desc(subtime_datetime)) %>% 
  dplyr::mutate(rank_newpage = dplyr::row_number()) %>% 
  dplyr::arrange(samptime_coarse) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(on_toppage = !is.na(rank_toppage)) -> data_new_page_ranks

data_new_page_ranks %>% 
  dplyr::filter(rank_newpage == 31) %>%
  dplyr::filter(score == 1) %>% 
  dplyr::select(id) %>% 
  unique() -> id_filter

data_new_page_ranks %>% 
  dplyr::filter(rank_newpage > 30) %>% 
  dplyr::inner_join(id_filter, by = "id") -> after_new_page

```


Distribution of maximum scores reached if the story did not pick up on the `New` page.

```{r}

after_new_page %>% 
  dplyr::group_by(id) %>% 
  dplyr::summarise(max_score_after_new = max(score)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(max_score_after_new) %>% 
  dplyr::group_by(max_score_after_new) %>% 
  dplyr::summarise(count = n()) %>% 
  dplyr::ungroup() -> max_score_after_new_page

max_score_after_new_page %>% 
  ggplot(aes(x = max_score_after_new, y = count)) +
  # geom_line() +
  geom_bar(stat = "identity", width = 0.1) +
  # geom_point(size = 2) +
  scale_x_log10(breaks = c(1, 2, 3, 4, 5, 10, 30, 100)) +
  theme(panel.grid.minor.x = element_blank())

```

The overwhelming majority of the stories that did not get any votes after passing through the `New` page do not pick up anymore.

```{r}

n_stories <- sum(max_score_after_new_page$count)

max_score_after_new_page %>% 
  mutate(fraction = count / n_stories) %>% 
  mutate(cum_fraction = cumsum(fraction)) -> cumulated_fractions

cumulated_fractions

```

About 62.4 % of these stories do not even make it past a score of 1, 85.9 % past a score of 2 and 92.9 % past a score of 3


The following data shows what happens with all stories for comparison.

```{r}

n_stories <- dim(data %>% select(id) %>% unique())[1]

data %>% 
  group_by(id) %>% 
  summarise(max_score = max(score)) %>% 
  ungroup() %>% 
  group_by(max_score) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(fraction = count / n_stories) %>% 
  mutate(cum_fraction = cumsum(fraction)) -> max_scores_by_id

```




Some stories still pick up traction, even if they did not get any on the `New` front page.

```{r}

after_new_page_sample <- get_random_sample(after_new_page, 30)

after_new_page %>% 
  ggplot(aes(x = samptime_datetime, y = score)) +
  geom_jitter(alpha = 0.2) +
  scale_y_log10()

```







# Do all stories make it to the `Top` page at some point?



```{r}

data %>% 
  filter(!is.na(rank_toppage)) %>% 
  select(id) %>% 
  unique() -> top_story_achievers

data %>% 
  filter(rank_toppage <= 100) %>% 
  select(id) %>% 
  unique() -> top_100_achievers

data %>% 
  filter(rank_toppage <= 30) %>% 
  select(id) %>% 
  unique() -> top_30_achievers

data %>% 
  select(id) %>% 
  unique() -> all_ids


percentage_top_story_achievers <- dim(top_story_achievers)[1] / dim(all_ids)[1]
percentage_top_100_achievers <- dim(top_100_achievers)[1] / dim(all_ids)[1]
percentage_top_30_achievers <- dim(top_30_achievers)[1] / dim(all_ids)[1]

percentage_top_story_achievers
percentage_top_100_achievers
percentage_top_30_achievers
```





# Distribution of Ages on the Top Page


```{r}
max_age_top_page <- data %>% 
  filter(!is.na(rank_toppage)) %>% 
  select(age_hours) %>% 
  max() / 24
  
```




Next, we will present the distribution of the ages on the top page. First of all, the distribution of ages on the top page in general does not reveal anything surprising: There are few *very* new stories on the top page, then there seems to be an optimal age, where stories have gathered enough votes, but are not yet so old that the age penalty pushes them out of the top page. For higher ages, the number of stories declines rapidly and there are only very few stories that are older than a few days. In fact, the oldest story on the top page in the entire data set is `r max_age_top_page` days old.

```{r}
data %>% 
  filter(!is.na(rank_toppage)) %>% 
  ggplot(aes(x = age_hours)) +
  geom_histogram(fill = "black", alpha = 0.5) +
  scale_y_continuous(labels = c("0", "10k", "20k", "30k", "40k")) +
  labs(title = "Distribution of Ages of Stories in the Top 500",
       x = "Age [Hours]",
       y = "Count") +
  theme_hn() 
```


For stories on the front page of the top list, this effect is -- also unsurprisingly -- even more extreme.

```{r}

data %>% 
  filter(!is.na(rank_toppage)) %>%  
  filter(rank_toppage <= 30) %>% 
  ggplot(aes(x = age_hours)) +
  geom_histogram(fill = "black", alpha = 0.5) +
  scale_y_continuous(labels = c("0", "20k", "40k", "60k")) +
  labs(title = "Distribution of Ages of Stories in the Top 30",
       x = "Age [Hours]",
       y = "Count") +
  theme_hn() 

```

Please note that this is an aggregation over all top lists over all sample times. 


```{r, eval=FALSE}

# TODO: IS THIS ANIMATION REALLY NECESSARY?
# plot distributions of ages on toppage
data %>% 
  filter(!is.na(rank_toppage)) %>% 
  filter(rank_toppage <= 30) %>% 
  ggplot(aes(x = age / 60 / 60)) +
  geom_histogram(fill = "white", alpha = 0.5) +
  theme_hn() -> p

# add animation
p +
  transition_states(samptime_d, 
                    transition_length = 1, 
                    state_length = 1) -> anim

# render/show animation
# anim

```


Oldest story on the Top Page

```{r}
# get maximum age of story up to specified rank
get_max_age <- function(max_rank, data) {
  data %>% 
    filter(!is.na(rank_toppage)) %>% 
    filter(rank_toppage <= max_rank) %>%
    select(age_hours) %>% 
    max()  
}

# compute max age up to rank for 1:500
max_age_by_rank <- tibble(up_to_rank = seq(1, 500, by = 1),
                          max_age = sapply(seq(1, 500, by = 1), FUN = get_max_age, data = data))  # takes long to compute...

max_age_by_rank %>% knitr::kable()
```


```{r}
# plot max age by rank
max_age_by_rank %>% 
  ggplot(aes(x = up_to_rank, y = max_age)) +
  geom_line(size = 1) +
  scale_x_log10() +
  labs(title = "Oldest Story to ever make it to Specific Rank",
       x = "Rank",
       y = "Age of Oldest Story [Hours]") +
  theme_hn()

```








Minimum score to make it to the top page.

```{r}

data %>% 
  select(id, score, rank_toppage, samptime_datetime) %>% 
  filter(!is.na(rank_toppage)) %>% 
  dplyr::mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  group_by(score) %>% 
  summarize(count = n()) %>% 
  ungroup() -> top_page_scores


top_page_scores

top_page_scores %>% 
  filter(score <= 10) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(1, 10, by = 1)) +
  theme(panel.grid.minor.x = element_blank())


top_page_scores %>% 
  filter((score > 10) & (score <= 100)) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(10, 100, by = 10)) +
  theme(panel.grid.minor.x = element_blank())

top_page_scores %>% 
  filter((score > 100) & (score <= 1000)) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(100, 1000, by = 100)) +
  theme(panel.grid.minor.x = element_blank())

```

There is a significant amount of stories with score 1 that make it to the top page. After that, the distribution becomes somewhat strange. There are areas of density and areas where there are no stories at all.













About 22.7 % of all stories make it to the top page.






























