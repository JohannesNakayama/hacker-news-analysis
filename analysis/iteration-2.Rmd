---
title: "Hacker News Analysis"
output: 
  prettydoc::html_pretty:
    theme: tactile
---

`r lubridate::today()`

```{r setup, include=FALSE}

# defaults and libraries
knitr::opts_chunk$set(echo = FALSE, message = FALSE)
library(dplyr)
library(ggplot2)
library(magrittr)
library(gganimate)
source("helpers.R")

```








# Data Formatting


```{r read-data}

# if processed data is not in data folder: process, save and load
# else: load processed data
if (!("hn-data-processed.rds" %in% list.files(file.path("..", "data")))) {
  data <- read.table(file.path("..", "data", "newstories_2021-01-26_14-32-18.tsv"), sep = "\t", header = TRUE)
  data %<>% 
    mutate(rank = as.numeric(rank),
           age_seconds = sample_time - submission_time) %>% 
    mutate(age_hours = age_seconds / 60 / 60) %>% 
    rename(rank_toppage = rank) %>% 
    mutate(subtime_datetime = lubridate::as_datetime(submission_time, tz = "CET"),
                  samptime_datetime = lubridate::as_datetime(sample_time, tz = "CET")) %>% 
    # dplyr::mutate(subtime_y = lubridate::year(subtime_datetime),
    #               subtime_m = lubridate::month(subtime_datetime),
    #               subtime_d = lubridate::day(subtime_datetime),
    #               subtime_h = lubridate::hour(subtime_datetime),
    #               subtime_m = lubridate::minute(subtime_datetime),
    #               subtime_s = lubridate::second(subtime_datetime),
    #               samptime_y = lubridate::year(samptime_datetime),
    #               samptime_m = lubridate::month(samptime_datetime),
    #               samptime_d = lubridate::day(samptime_datetime),
    #               samptime_h = lubridate::hour(samptime_datetime),
    #               samptime_m = lubridate::minute(samptime_datetime),
    #               samptime_s = lubridate::second(samptime_datetime)) %>%
    mutate(subtime_clocktime = update(subtime_datetime, yday = 1)) %>% 
    select(-c(submission_time, sample_time))
  readr::write_rds(data, file.path("..", "data", "hn-data-processed.rds"), compress = "gz")  
} else {
  data <- readr::read_rds(file.path("..", "data", "hn-data-processed.rds"))  
}

```

```{r anonymization}

# for publication: replace id column with this anonymous id column (via inner_join)
data %>% 
  select(id) %>% 
  unique() %>% 
  arrange(id) %>% 
  mutate(anon_id = row_number()) -> id_set

```

```{r helper-functions}

# return data frame with IDs of new stories during scraping
get_id_selector <- function(data) {
  data %>% 
    # TODO: DECIDE WHAT TO DO ABOUT STORIES THAT WERE ALREADY THERE
    # dplyr::filter(age < 120) %>% 
    select(id) %>% 
    unique() -> id_selector  
  return(id_selector)
}

# subset the data by a random sample of IDs
get_random_sample <- function(data, n) {
  id_selector <- get_id_selector(data)
  if (dim(id_selector)[1] < n) {
    stop("n is too large")
  }
  data %>% inner_join(sample_n(id_selector, n), by = "id") -> random_sample
  return(random_sample)
}

# subset the data by the first n IDs that occurred 
get_first_n <- function(data, n) {
  id_selector <- get_id_selector(data)
  if (dim(id_selector)[1] < n) {
    stop("n is too large")
  }
  first_n_sample <- data %>% inner_join(head(id_selector, n), by = "id")  
  return(first_n_sample)
}

```

```{r, message=TRUE}

# test functions
data %>% get_first_n(100)
data %>% get_random_sample(100)

```









# Distribution of Submissions over Time


During the entire timeframe of our sampling, a clear regularity in terms of posting volume can be observed. 

```{r kde-posting-volume-datetime}

data %>% 
  ggplot(aes(x = subtime_datetime)) +
  geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
  labs(title = "KDE of Posting Volume",
       x = "Submission Time",
       y = "KDE",
       caption = "The red dashed line indicates the start of the sampling.") +
  geom_vline(xintercept = min(data$samptime_datetime), size = 1, linetype = "dashed", color = "red") +
  theme_hn() +
  theme(axis.text.y = element_blank())

```


Looking into this pattern, we can see that (from the perspective of Central European Time) there is low traffic in the early morning, then it picks up and increases until it reaches its peak at around 6 pm.

```{r kde-posting-volume-time}

data %>% 
  ggplot(aes(x = subtime_clocktime)) +
  geom_density(color = "black", fill = "black", alpha = 0.5, size = 1) +
  labs(title = "KDE of Posting Volume by Time of Day",
       x = "Time of day",
       y = "KDE",
       caption = "The displayed times are in CET.") +
  scale_x_datetime(date_labels = "%H:%M:%S") +
  theme_hn() +
  theme(axis.text.y = element_blank())

```


Lastly, we can look into how the arrival rates of new stories are distributed. The obvious hypothesis would be to assume a Poisson distribution.

```{r format-data-for-submission-times}
# get IDs and submission times and sort them by temporally
data %>% 
  select(id, subtime_datetime) %>% 
  unique() %>% 
  arrange(subtime_datetime) %>% 
  select(subtime_datetime) -> sorted_submission_times

# compute for every 1 minute timeslice how many new stories are posted
tibble(subtime = sorted_submission_times$subtime_datetime, 
       bin = cut(sorted_submission_times$subtime_datetime, 
                 breaks = "1 min", 
                 labels = FALSE)) %>% 
  mutate(bin = factor(bin, levels = seq(1, max(bin), by = 1))) %>% 
  group_by(bin, .drop = FALSE) %>% 
  summarize(arr_count = n()) -> arrival_counts
```




```{r plot-submission-times-distributions}
# generics
y_max <- 7000
rate_max <- 7

# show real distribution of arrival times
p1 <- arrival_counts %>% 
  ggplot(aes(x = arr_count)) +
  geom_bar(color = "transparent", fill = "black", alpha = 0.8) +
  ylim(c(0, y_max)) +
  scale_x_continuous(breaks = seq(0, 7, by = 1), limits = c(-0.5, rate_max + 0.5)) +
  labs(title = "Distribution of Arrival Rate per Minute (Empirical)",
       x = "Arrival Rate",
       y = "Count") +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

# simulate this process
set.seed(1)
n <- dim(arrival_counts)[1]
lambda <- mean(arrival_counts$arr_count)
simulated_distribution <- tibble(arr_count = rpois(n, lambda))

# show simulated distribution of arrival times
p2 <- simulated_distribution %>% 
  ggplot(aes(x = arr_count)) +
  geom_bar(color = "transparent", fill = "black", alpha = 0.8) +
  ylim(c(0, y_max)) +
  scale_x_continuous(breaks = seq(0, 7, by = 1), limits = c(-0.5, rate_max + 0.5)) +
  labs(title = "Distribution of Arrival Rate per Minute (Simulated)",
       x = "Arrival Rate",
       y = "Count")  +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())

ggpubr::ggarrange(p1, p2, ncol = 1)

```

As can be seen, arrival rates can be simulated with this fitted Poisson process very accurately. If we can determine the user behavior equally well, we can find a good mapping between real-world time and simulation time.





# How do stories develop on the `New` page?





The next plot shows how the scores of stories develops while they are on the new page.

```{r, eval=FALSE}
data %>% 
  select(id, age_seconds, score, rank_toppage, subtime_datetime, samptime_datetime) %>%
  mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  group_by(samptime_coarse) %>%
  slice_max(n = 200, order_by = subtime_datetime) %>%
  mutate(rank_newpage = row_number()) %>% 
  ungroup() %>% 
  arrange(samptime_coarse) %>% 
  mutate(on_toppage = !is.na(rank_toppage)) -> newpage_moving_window
```


TODO: ANALYSE HOW MANY VOTES STORIES GET BEFORE THEY GET TO THE FRONT PAGE (AGGREGATE OVER ALL STORIES ON THE NEW PAGE).

```{r, eval=FALSE}
color_scheme <- c("FALSE" = "darkgrey", "TRUE" = "red")
set.seed(123)
newpage_moving_window %>% 
  get_random_sample(100) %>% 
  ggplot(aes(x = rank_newpage, y = score, group = id, color = on_toppage)) +
  geom_jitter(alpha = 0.1, height = 0.05) +
  scale_x_continuous(trans = "reverse") +
  scale_color_manual(values = color_scheme) +
  labs(title = "How do stories emerge from the new page to the top page?",
       x = "Newpage Rank",
       y = "Score", 
       color = "On Toppage") +
  coord_flip() +
  theme_hn() +
  theme(panel.grid.major.x = element_blank(),
        panel.grid.minor.x = element_blank(),
        panel.grid.minor.y = element_blank())
```

```{r}

color_scheme <- c("FALSE" = "darkgrey", "TRUE" = "red")

set.seed(123)

newpage_moving_window %>% 
  get_random_sample(100) %>% 
  filter(age_seconds <= 50000) %>%
  ggplot(aes(x = age_seconds / 3600, y = score, group = id, color = on_toppage)) +
  # geom_jitter(alpha = 0.2, height = 0.05) +
  geom_point(alpha = 0.2) +
  scale_color_manual(values = color_scheme) +
  # scale_x_continuous(trans = "reverse") +
  scale_y_log10() +
  theme_hn() +
  NULL
```

```{r}

# TODO: COMPUTE HOW MANY VOTES STORIES GET WHEN THEY ARE NOT ON THE TOP PAGE

newpage_moving_window %>% 
  group_by(id) %>% 
  slice(if(any(on_toppage == FALSE)) 1:which.max(on_toppage == FALSE) else row_number())
  
  
```







How many votes do stories gain while on the `new` page?

```{r}

newpage_moving_window %>% 
  group_by(id) %>% 
  summarize(gained_votes = max(score) - 1) %>% 
  ungroup() -> gained_votes_per_id_newpage

gained_votes_per_id_newpage %>% 
  filter(!is.na(gained_votes)) -> gained_votes_per_id_newpage_nondeleted

gained_votes_per_id_newpage_nondeleted$gained_votes %>% mean()


gained_votes_per_id_newpage %>% 
  filter(!is.na(gained_votes)) %>% 
  group_by(gained_votes) %>% 
  summarize(count = n()) %>% 
  filter(gained_votes <= 100) %>% 
  ggplot(aes(x = gained_votes, y = count)) +
  geom_bar(stat = "identity")

```

Each story seems to gain 17.4 votes while on the `new` page on average. Whether or not that is due to being there is not clear. This has to be contrasted with what happens when the stories goes further down on the `new` page.


```{r}

data %>% 
  filter(id == 25870990)

```

Some stories seem to be deleted after some time, resulting in a score NA (see table above).








# Do stories get upvotes if they pass the `New` page without getting any upvotes?


```{r}
data %>% 
  dplyr::select(id, score, rank_toppage, subtime_datetime, samptime_datetime) %>%
  dplyr::rename(rank_toppage = rank) %>% 
  dplyr::mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  dplyr::group_by(samptime_coarse) %>%
  dplyr::arrange(desc(subtime_datetime)) %>% 
  dplyr::mutate(rank_newpage = dplyr::row_number()) %>% 
  dplyr::arrange(samptime_coarse) %>% 
  dplyr::ungroup() %>% 
  dplyr::mutate(on_toppage = !is.na(rank_toppage)) -> data_new_page_ranks

data_new_page_ranks %>% 
  dplyr::filter(rank_newpage == 31) %>%
  dplyr::filter(score == 1) %>% 
  dplyr::select(id) %>% 
  unique() -> id_filter

data_new_page_ranks %>% 
  dplyr::filter(rank_newpage > 30) %>% 
  dplyr::inner_join(id_filter, by = "id") -> after_new_page

```


Distribution of maximum scores reached if the story did not pick up on the `New` page.

```{r}

after_new_page %>% 
  dplyr::group_by(id) %>% 
  dplyr::summarise(max_score_after_new = max(score)) %>% 
  dplyr::ungroup() %>% 
  dplyr::select(max_score_after_new) %>% 
  dplyr::group_by(max_score_after_new) %>% 
  dplyr::summarise(count = n()) %>% 
  dplyr::ungroup() -> max_score_after_new_page

max_score_after_new_page %>% 
  ggplot(aes(x = max_score_after_new, y = count)) +
  # geom_line() +
  geom_bar(stat = "identity", width = 0.1) +
  # geom_point(size = 2) +
  scale_x_log10(breaks = c(1, 2, 3, 4, 5, 10, 30, 100)) +
  theme(panel.grid.minor.x = element_blank())

```

The overwhelming majority of the stories that did not get any votes after passing through the `New` page do not pick up anymore.

```{r}

n_stories <- sum(max_score_after_new_page$count)

max_score_after_new_page %>% 
  mutate(fraction = count / n_stories) %>% 
  mutate(cum_fraction = cumsum(fraction)) -> cumulated_fractions

cumulated_fractions

```

About 62.4 % of these stories do not even make it past a score of 1, 85.9 % past a score of 2 and 92.9 % past a score of 3


The following data shows what happens with all stories for comparison.

```{r}

n_stories <- dim(data %>% select(id) %>% unique())[1]

data %>% 
  group_by(id) %>% 
  summarise(max_score = max(score)) %>% 
  ungroup() %>% 
  group_by(max_score) %>% 
  summarise(count = n()) %>% 
  ungroup() %>% 
  mutate(fraction = count / n_stories) %>% 
  mutate(cum_fraction = cumsum(fraction)) -> max_scores_by_id

```




Some stories still pick up traction, even if they did not get any on the `New` front page.

```{r}

after_new_page_sample <- get_random_sample(after_new_page, 30)

after_new_page %>% 
  ggplot(aes(x = samptime_datetime, y = score)) +
  geom_jitter(alpha = 0.2) +
  scale_y_log10()

```







# Do all stories make it to the `Top` page at some point?



```{r}

data %>% 
  filter(!is.na(rank_toppage)) %>% 
  select(id) %>% 
  unique() -> top_story_achievers

data %>% 
  filter(rank_toppage <= 100) %>% 
  select(id) %>% 
  unique() -> top_100_achievers

data %>% 
  filter(rank_toppage <= 30) %>% 
  select(id) %>% 
  unique() -> top_30_achievers

data %>% 
  select(id) %>% 
  unique() -> all_ids


percentage_top_story_achievers <- dim(top_story_achievers)[1] / dim(all_ids)[1]
percentage_top_100_achievers <- dim(top_100_achievers)[1] / dim(all_ids)[1]
percentage_top_30_achievers <- dim(top_30_achievers)[1] / dim(all_ids)[1]

percentage_top_story_achievers
percentage_top_100_achievers
percentage_top_30_achievers
```





# Distribution of Ages on the Top Page


```{r}
max_age_top_page <- data %>% 
  filter(!is.na(rank_toppage)) %>% 
  select(age_hours) %>% 
  max() / 24
  
```




Next, we will present the distribution of the ages on the top page. First of all, the distribution of ages on the top page in general does not reveal anything surprising: There are few *very* new stories on the top page, then there seems to be an optimal age, where stories have gathered enough votes, but are not yet so old that the age penalty pushes them out of the top page. For higher ages, the number of stories declines rapidly and there are only very few stories that are older than a few days. In fact, the oldest story on the top page in the entire data set is `r max_age_top_page` days old.

```{r}
data %>% 
  filter(!is.na(rank_toppage)) %>% 
  ggplot(aes(x = age_hours)) +
  geom_histogram(fill = "black", alpha = 0.5) +
  scale_y_continuous(labels = c("0", "10k", "20k", "30k", "40k")) +
  labs(title = "Distribution of Ages of Stories in the Top 500",
       x = "Age [Hours]",
       y = "Count") +
  theme_hn() 
```


For stories on the front page of the top list, this effect is -- also unsurprisingly -- even more extreme.

```{r}

data %>% 
  filter(!is.na(rank_toppage)) %>%  
  filter(rank_toppage <= 30) %>% 
  ggplot(aes(x = age_hours)) +
  geom_histogram(fill = "black", alpha = 0.5) +
  scale_y_continuous(labels = c("0", "20k", "40k", "60k")) +
  labs(title = "Distribution of Ages of Stories in the Top 30",
       x = "Age [Hours]",
       y = "Count") +
  theme_hn() 

```

Please note that this is an aggregation over all top lists over all sample times. 


```{r, eval=FALSE}

# TODO: IS THIS ANIMATION REALLY NECESSARY?
# plot distributions of ages on toppage
data %>% 
  filter(!is.na(rank_toppage)) %>% 
  filter(rank_toppage <= 30) %>% 
  ggplot(aes(x = age / 60 / 60)) +
  geom_histogram(fill = "white", alpha = 0.5) +
  theme_hn() -> p

# add animation
p +
  transition_states(samptime_d, 
                    transition_length = 1, 
                    state_length = 1) -> anim

# render/show animation
# anim

```


Oldest story on the Top Page

```{r}
# get maximum age of story up to specified rank
get_max_age <- function(max_rank, data) {
  data %>% 
    filter(!is.na(rank_toppage)) %>% 
    filter(rank_toppage <= max_rank) %>%
    select(age_hours) %>% 
    max()  
}

# compute max age up to rank for 1:500
max_age_by_rank <- tibble(up_to_rank = seq(1, 500, by = 1),
                          max_age = sapply(seq(1, 500, by = 1), FUN = get_max_age, data = data))  # takes long to compute...

max_age_by_rank %>% knitr::kable()
```


```{r}
# plot max age by rank
max_age_by_rank %>% 
  ggplot(aes(x = up_to_rank, y = max_age)) +
  geom_line(size = 1) +
  scale_x_log10() +
  labs(title = "Oldest Story to ever make it to Specific Rank",
       x = "Rank",
       y = "Age of Oldest Story [Hours]") +
  theme_hn()

```








Minimum score to make it to the top page.

```{r}

data %>% 
  select(id, score, rank_toppage, samptime_datetime) %>% 
  filter(!is.na(rank_toppage)) %>% 
  dplyr::mutate(samptime_coarse = lubridate::round_date(samptime_datetime, unit = "minute")) %>% 
  group_by(score) %>% 
  summarize(count = n()) %>% 
  ungroup() -> top_page_scores


top_page_scores

top_page_scores %>% 
  filter(score <= 10) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(1, 10, by = 1)) +
  theme(panel.grid.minor.x = element_blank())


top_page_scores %>% 
  filter((score > 10) & (score <= 100)) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(10, 100, by = 10)) +
  theme(panel.grid.minor.x = element_blank())

top_page_scores %>% 
  filter((score > 100) & (score <= 1000)) %>% 
  ggplot(aes(x = score, y = count)) +
  geom_bar(stat = "identity") +
  scale_x_continuous(breaks = seq(100, 1000, by = 100)) +
  theme(panel.grid.minor.x = element_blank())

```

There is a significant amount of stories with score 1 that make it to the top page. After that, the distribution becomes somewhat strange. There are areas of density and areas where there are no stories at all.













About 22.7 % of all stories make it to the top page.




























